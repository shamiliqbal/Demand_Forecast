{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497c74d7-640a-417e-b902-0b3659b91b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 0Ô∏è ‚Äì IMPORT MODULES\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369be783-8f81-4bc9-b9b7-e812f0975265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 1Ô∏è ‚Äì DEFINE PROJECT FOLDER STRUCTURE\n",
    "# ==========================================================\n",
    "\n",
    "base_path = r\"C:\\Users\\shamil.iqbal\\Downloads\\Todays Reports\"\n",
    "\n",
    "sales_folder = os.path.join(base_path, \"CSV\")\n",
    "reference_folder = os.path.join(base_path, \"Reference\")\n",
    "processed_folder = os.path.join(base_path, \"Processed\")\n",
    "\n",
    "header_path = os.path.join(reference_folder, \"Header.csv\")\n",
    "outl_path = os.path.join(reference_folder, \"OUTL.csv\")\n",
    "\n",
    "print(\"Paths defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357da560-f2e2-4cce-af90-26bdfa6cb51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header column count: 87\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 2Ô∏è‚É£ ‚Äì LOAD HEADER FILE\n",
    "# ==========================================================\n",
    "\n",
    "header_cols = pd.read_csv(\n",
    "    header_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    encoding_errors=\"ignore\"\n",
    ").iloc[0].tolist()\n",
    "\n",
    "header_cols = [str(col).strip() for col in header_cols]\n",
    "\n",
    "print(\"Header column count:\", len(header_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adeefa25-92df-43ba-8353-729bf7dbf1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns found.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 3Ô∏è‚É£ ‚Äì DEFINE REQUIRED SALES COLUMNS\n",
    "# ==========================================================\n",
    "\n",
    "desired_cols = [\n",
    "    'Company', 'CompName',\n",
    "    'Store','StoreName','SKU','SkuDesc','SkuStatus',\n",
    "    'Supplier','SupplierName','SupplierType',\n",
    "    'Dept.','DeptName','SubDept.','SubDeptName',\n",
    "    'Class','ClassName','SubClass','SubClassName',\n",
    "    'Brand','BrandName',\n",
    "    'NetVndrUnitCost','SOHQty','InvValue',\n",
    "    'POonOrdrQty','TransferonOrderQty',\n",
    "    '30DaysQtySold','60DaysQtySold','90DaysQtySold'\n",
    "]\n",
    "\n",
    "missing = [c for c in desired_cols if c not in header_cols]\n",
    "if missing:\n",
    "    print(\"‚ö† Missing columns:\", missing)\n",
    "else:\n",
    "    print(\"All required columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd03c1a-f163-424f-a628-15787e9e660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date extraction function ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 4Ô∏è‚É£ ‚Äì FUNCTION: EXTRACT REPORT DATE FROM FILENAME\n",
    "# ==========================================================\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    match = re.search(r'(\\d{8})', filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return datetime.datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "    return None\n",
    "\n",
    "print(\"Date extraction function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bf2b2b-3336-4883-aca3-1cc5332725c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales files found: ['SKUDTLBPF_20260226-103743.csv', 'SKUDTLIPF_20260226-103743.csv', 'SKUDTLPF_20260226-103743.csv']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 5Ô∏è‚É£ ‚Äì IDENTIFY SALES FILES\n",
    "# ==========================================================\n",
    "\n",
    "data_files = [\n",
    "    f for f in os.listdir(sales_folder)\n",
    "    if f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "print(\"Sales files found:\", data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194e869f-926e-4b51-a5ec-759d0f224319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing 1/3: SKUDTLBPF_20260226-103743.csv\n",
      "   Report Date: 2026-02-26\n",
      "   Loaded 300,000 rows (file) | 300,000 rows (total)\n",
      "   Loaded 544,216 rows (file) | 544,216 rows (total)\n",
      "   ‚úÖ Finished SKUDTLBPF_20260226-103743.csv in 4.62 seconds\n",
      "\n",
      "üîÑ Processing 2/3: SKUDTLIPF_20260226-103743.csv\n",
      "   Report Date: 2026-02-26\n",
      "   Loaded 300,000 rows (file) | 844,216 rows (total)\n",
      "   Loaded 600,000 rows (file) | 1,144,216 rows (total)\n",
      "   Loaded 900,000 rows (file) | 1,444,216 rows (total)\n",
      "   Loaded 1,200,000 rows (file) | 1,744,216 rows (total)\n",
      "   Loaded 1,500,000 rows (file) | 2,044,216 rows (total)\n",
      "   Loaded 1,800,000 rows (file) | 2,344,216 rows (total)\n",
      "   Loaded 1,856,754 rows (file) | 2,400,970 rows (total)\n",
      "   ‚úÖ Finished SKUDTLIPF_20260226-103743.csv in 14.26 seconds\n",
      "\n",
      "üîÑ Processing 3/3: SKUDTLPF_20260226-103743.csv\n",
      "   Report Date: 2026-02-26\n",
      "   Loaded 300,000 rows (file) | 2,700,970 rows (total)\n",
      "   Loaded 600,000 rows (file) | 3,000,970 rows (total)\n",
      "   Loaded 900,000 rows (file) | 3,300,970 rows (total)\n",
      "   Loaded 1,200,000 rows (file) | 3,600,970 rows (total)\n",
      "   Loaded 1,500,000 rows (file) | 3,900,970 rows (total)\n",
      "   Loaded 1,800,000 rows (file) | 4,200,970 rows (total)\n",
      "   Loaded 2,100,000 rows (file) | 4,500,970 rows (total)\n",
      "   Loaded 2,400,000 rows (file) | 4,800,970 rows (total)\n",
      "   Loaded 2,700,000 rows (file) | 5,100,970 rows (total)\n",
      "   Loaded 2,917,595 rows (file) | 5,318,565 rows (total)\n",
      "   ‚úÖ Finished SKUDTLPF_20260226-103743.csv in 23.62 seconds\n",
      "\n",
      "üîÑ Combining all sales files...\n",
      "‚úÖ Final Sales Shape: (5318565, 30)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 6Ô∏è ‚Äì LOAD SALES FILES (HEAVY PROCESS)\n",
    "# ==========================================================\n",
    "\n",
    "chunksize = 300_000\n",
    "all_chunks = []\n",
    "grand_total = 0\n",
    "\n",
    "for i, file in enumerate(data_files, start=1):\n",
    "\n",
    "    file_path = os.path.join(sales_folder, file)\n",
    "    report_date = extract_date_from_filename(file)\n",
    "\n",
    "    print(f\"\\nüîÑ Processing {i}/{len(data_files)}: {file}\")\n",
    "    print(f\"   Report Date: {report_date}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    file_total = 0\n",
    "\n",
    "    for chunk in pd.read_csv(\n",
    "        file_path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=header_cols,\n",
    "        usecols=desired_cols,\n",
    "        chunksize=chunksize,\n",
    "        encoding_errors=\"ignore\",\n",
    "        low_memory=False\n",
    "    ):\n",
    "        file_total += len(chunk)\n",
    "        grand_total += len(chunk)\n",
    "\n",
    "        chunk[\"ReportCreationDate\"] = report_date\n",
    "        chunk[\"Source_File\"] = file\n",
    "\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "        print(f\"   Loaded {file_total:,} rows (file) | {grand_total:,} rows (total)\")\n",
    "\n",
    "    print(f\"   ‚úÖ Finished {file} in {round(time.time() - start_time, 2)} seconds\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BLOCK 7Ô∏è‚É£ A ‚Äì COMBINE SALES DATA\n",
    "# ==========================================================\n",
    "\n",
    "print(\"\\nüîÑ Combining all sales files...\")\n",
    "combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "print(\"‚úÖ Final Sales Shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca185f5b-0363-443e-a10b-9712650c9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtered Shape: (17731, 30)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 7Ô∏è‚É£ B ‚Äì FILTER DATA (Company 90 + SupplierType L + Dept list)\n",
    "# ==========================================================\n",
    "\n",
    "# Make filters robust (handle numbers stored as text / floats)\n",
    "combined_df[\"Company\"] = combined_df[\"Company\"].astype(str).str.strip()\n",
    "combined_df[\"CompName\"] = combined_df[\"CompName\"].astype(str).str.strip()\n",
    "combined_df[\"SupplierType\"] = combined_df[\"SupplierType\"].astype(str).str.strip()\n",
    "\n",
    "# Dept. sometimes comes as 600 or 600.0, so convert to numeric safely\n",
    "combined_df[\"Dept.\"] = pd.to_numeric(combined_df[\"Dept.\"], errors=\"coerce\")\n",
    "\n",
    "dept_list = [600, 605, 625, 630, 635, 645]\n",
    "\n",
    "filtered_df = combined_df[\n",
    "    (combined_df[\"Company\"] == \"90\") &\n",
    "    (combined_df[\"CompName\"] == \"DANUBE COMPANY - BAHRAIN\") &\n",
    "    (combined_df[\"SupplierType\"] == \"L\") &\n",
    "    (combined_df[\"Dept.\"].isin(dept_list))\n",
    "].copy()\n",
    "\n",
    "print(\"‚úÖ Filtered Shape:\", filtered_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14aa1c58-cfc5-40f3-9a79-7d586d03e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeptHierarchy column created.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 8Ô∏è‚É£ ‚Äì CREATE DEPT HIERARCHY COLUMN\n",
    "# ==========================================================\n",
    "\n",
    "filtered_df[\"DeptHierarchy\"] = (\n",
    "    filtered_df[[\"Dept.\", \"SubDept.\", \"Class\", \"SubClass\"]]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .agg(\"-\".join, axis=1)\n",
    ")\n",
    "\n",
    "print(\"DeptHierarchy column created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787517b6-7304-41a8-aa85-7a61a18d6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand metrics calculated successfully.\n",
      "Total Daily Avg Sales: 1561.8\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 9Ô∏è‚É£ ‚Äì DEMAND CALCULATIONS (30/60/90 LOGIC)\n",
    "# ==========================================================\n",
    "\n",
    "# Ensure sales columns are numeric\n",
    "sales_cols = [\"30DaysQtySold\", \"60DaysQtySold\", \"90DaysQtySold\"]\n",
    "\n",
    "for col in sales_cols:\n",
    "    filtered_df[col] = pd.to_numeric(\n",
    "        filtered_df[col],\n",
    "        errors=\"coerce\"\n",
    "    ).fillna(0)\n",
    "\n",
    "# =MAX(AY3,0)\n",
    "filtered_df[\"Adj_30Days\"] = (\n",
    "    filtered_df[\"30DaysQtySold\"].clip(lower=0)\n",
    ")\n",
    "\n",
    "# =MAX(BB3-AY3,0)\n",
    "filtered_df[\"Adj_60Days\"] = (\n",
    "    filtered_df[\"60DaysQtySold\"] -\n",
    "    filtered_df[\"30DaysQtySold\"]\n",
    ").clip(lower=0)\n",
    "\n",
    "# =MAX(BE3-BB3,0)\n",
    "filtered_df[\"Adj_90Days\"] = (\n",
    "    filtered_df[\"90DaysQtySold\"] -\n",
    "    filtered_df[\"60DaysQtySold\"]\n",
    ").clip(lower=0)\n",
    "\n",
    "# =AVERAGE(CJ3:CL3)\n",
    "filtered_df[\"Avg_30_60_90\"] = (\n",
    "    filtered_df[[\"Adj_30Days\", \"Adj_60Days\", \"Adj_90Days\"]]\n",
    "    .mean(axis=1)\n",
    ")\n",
    "\n",
    "# Daily sales (ADF)\n",
    "filtered_df[\"Daily_Avg_Sales\"] = (\n",
    "    filtered_df[\"Avg_30_60_90\"] / 30\n",
    ")\n",
    "\n",
    "print(\"Demand metrics calculated successfully.\")\n",
    "\n",
    "total_daily_avg_sales = filtered_df[\"Daily_Avg_Sales\"].sum()\n",
    "\n",
    "print(\"Total Daily Avg Sales:\", round(total_daily_avg_sales, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9fff591-5e58-4d3a-9107-3bf79b9e0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OUTL file loaded successfully | Rows: 462 | Columns: 11\n",
      "‚úÖ Master file loaded successfully | Rows: 27637 | Columns: 2\n",
      "‚úÖ Unit Size file loaded successfully | Rows: 23881 | Columns: 3\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 10  ‚Äì LOAD OUTL (REFERENCE FILE - Outline Days)\n",
    "# ==========================================================\n",
    "\n",
    "outl_df = pd.read_csv(\n",
    "    outl_path,\n",
    "    sep=\",\",\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ OUTL file loaded successfully | Rows: {outl_df.shape[0]} | Columns: {outl_df.shape[1]}\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BLOCK 1Ô∏è‚É£1Ô∏è‚É£ ‚Äì LOAD MASTER & UNIT SIZE REFERENCE FILES\n",
    "# ==========================================================\n",
    "\n",
    "# Define file paths\n",
    "master_path = os.path.join(reference_folder, \"Master File Comments.csv\")\n",
    "unitsize_path = os.path.join(reference_folder, \"Unit Size.csv\")\n",
    "\n",
    "# Load Master file\n",
    "master_df = pd.read_csv(\n",
    "    master_path,\n",
    "    sep=\",\",              # adjust if needed\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Master file loaded successfully | Rows: {master_df.shape[0]} | Columns: {master_df.shape[1]}\")\n",
    "\n",
    "# Load Unit Size file\n",
    "unitsize_df = pd.read_csv(\n",
    "    unitsize_path,\n",
    "    sep=\",\",              # adjust if needed\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Unit Size file loaded successfully | Rows: {unitsize_df.shape[0]} | Columns: {unitsize_df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faadbbd3-1db2-4414-8cc8-dd63421247a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeptHierarchy not found in OUTL:\n",
      "<StringArray>\n",
      "['600-999-35-5', '600-80-20-10', '600-999-50-1', '600-80-30-15',\n",
      "   '600-5-3-16', '600-999-30-5',    '600-6-1-1',   '605-40-5-5',\n",
      "   '605-50-5-5', '625-999-15-5']\n",
      "Length: 10, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 12 ‚Äì MAP OUTL OUTL DAYS (Policy Lookup)\n",
    "# ==========================================================\n",
    "\n",
    "# Clean spaces\n",
    "filtered_df[\"DeptHierarchy\"] = filtered_df[\"DeptHierarchy\"].astype(str).str.strip()\n",
    "outl_df[\"Con.\"] = outl_df[\"Con.\"].astype(str).str.strip()\n",
    "\n",
    "# Ensure OUTL is numeric\n",
    "outl_df[\"901\"] = pd.to_numeric(outl_df[\"901\"], errors=\"coerce\")\n",
    "\n",
    "# Create lookup (Con. -> 901)\n",
    "outl_lookup = outl_df.drop_duplicates(\"Con.\").set_index(\"Con.\")[\"901\"]\n",
    "\n",
    "# Map WITHOUT default first\n",
    "mapped_days = filtered_df[\"DeptHierarchy\"].map(outl_lookup)\n",
    "\n",
    "# Identify true missing keys\n",
    "missing_keys = filtered_df.loc[\n",
    "    mapped_days.isna(), \"DeptHierarchy\"\n",
    "].unique()\n",
    "\n",
    "print(\"DeptHierarchy not found in OUTL:\")\n",
    "print(missing_keys)\n",
    "\n",
    "# Apply default AFTER detection\n",
    "filtered_df[\"OUTL_901_Days\"] = mapped_days.fillna(21)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a0819c1-ff17-4916-8fb5-9f64688fcdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OUTL_QTY calculated successfully.\n",
      "Total OUTL_QTY: 42618.46\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 12A ‚Äì CALCULATE OUTL_QTY (Daily Demand √ó OUTL Days)\n",
    "# ==========================================================\n",
    "\n",
    "filtered_df[\"OUTL_QTY\"] = (\n",
    "    filtered_df[\"Daily_Avg_Sales\"] *\n",
    "    filtered_df[\"OUTL_901_Days\"]\n",
    ")\n",
    "\n",
    "# Calculate total OUTL quantity\n",
    "total_outl_qty = filtered_df[\"OUTL_QTY\"].sum()\n",
    "\n",
    "print(\"‚úÖ OUTL_QTY calculated successfully.\")\n",
    "print(f\"Total OUTL_QTY: {round(total_outl_qty, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b77d407-d45b-4047-9f03-e4b9b8796dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Set_Qty mapped successfully.\n",
      "Defaulted to 1: 6473 out of 17731 rows\n",
      "Total SET_QTY: 270751.8\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 13 ‚Äì MAP SET QUANTITY USING PARENT SKU\n",
    "# ==========================================================\n",
    "\n",
    "# Clean\n",
    "filtered_df[\"SKU\"] = filtered_df[\"SKU\"].astype(str).str.strip()\n",
    "unitsize_df[\"Parent SKU\"] = unitsize_df[\"Parent SKU\"].astype(str).str.strip()\n",
    "\n",
    "unitsize_df[\"Set Quantity\"] = pd.to_numeric(unitsize_df[\"Set Quantity\"], errors=\"coerce\")\n",
    "\n",
    "# Lookup\n",
    "parent_lookup = (\n",
    "    unitsize_df\n",
    "    .drop_duplicates(\"Parent SKU\")\n",
    "    .set_index(\"Parent SKU\")[\"Set Quantity\"]\n",
    ")\n",
    "\n",
    "# Map\n",
    "filtered_df[\"Set_Qty\"] = filtered_df[\"SKU\"].map(parent_lookup)\n",
    "\n",
    "# Track missing before default\n",
    "filtered_df[\"Set_Qty_Missing\"] = filtered_df[\"Set_Qty\"].isna()\n",
    "\n",
    "# Apply default = 1\n",
    "filtered_df[\"Set_Qty\"] = filtered_df[\"Set_Qty\"].fillna(1)\n",
    "\n",
    "total_rows = len(filtered_df)\n",
    "defaulted_rows = filtered_df[\"Set_Qty_Missing\"].sum()\n",
    "\n",
    "print(\"‚úÖ Set_Qty mapped successfully.\")\n",
    "print(f\"Defaulted to 1: {defaulted_rows} out of {total_rows} rows\")\n",
    "\n",
    "# Calculate total SET quantity\n",
    "total_Set_Qty_qty = filtered_df[\"Set_Qty\"].sum()\n",
    "\n",
    "print(f\"Total SET_QTY: {round(total_Set_Qty_qty, 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9923b09a-ad8d-4a72-ba7b-12b133f73ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found: 207\n",
      "‚úÖ Duplicate Master SKU file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# EXPORT DUPLICATE Master SKUs TO EXCEL\n",
    "# ==========================================================\n",
    "\n",
    "dup_skus_df = master_df[\n",
    "    master_df.duplicated(subset=[\"SKU\"], keep=False)\n",
    "]\n",
    "\n",
    "print(\"Duplicate rows found:\", len(dup_skus_df))\n",
    "\n",
    "dup_output_path = os.path.join(base_path, \"Processed\", \"Duplicate_Master_SKUs.xlsx\")\n",
    "\n",
    "dup_skus_df.to_excel(dup_output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Duplicate Master SKU file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc64c9a-be97-4ab2-b2d7-ff37de364ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comments mapped with status condition.\n",
      "Missing Comments (Active only): 35 out of 17731 rows\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 14 ‚Äì MAP COMMENTS WITH STATUS CONDITION\n",
    "# ==========================================================\n",
    "\n",
    "# Clean SKUs\n",
    "filtered_df[\"SKU\"] = (\n",
    "    filtered_df[\"SKU\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "master_df[\"SKU\"] = (\n",
    "    master_df[\"SKU\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Build lookup: SKU -> Comment\n",
    "comments_lookup = (\n",
    "    master_df\n",
    "    .drop_duplicates(\"SKU\")\n",
    "    .set_index(\"SKU\")[\"Comment\"]\n",
    ")\n",
    "\n",
    "# --- CONDITIONAL MAPPING ---\n",
    "filtered_df[\"Comment\"] = filtered_df[\"SKU\"].map(comments_lookup)\n",
    "\n",
    "# If SkuStatus != \"A\", set \"Inactive\"\n",
    "filtered_df.loc[\n",
    "    filtered_df[\"SkuStatus\"] != \"A\",\n",
    "    \"Comment\"\n",
    "] = \"Inactive\"\n",
    "\n",
    "# Missing tracking (only for active SKUs)\n",
    "total_rows = len(filtered_df)\n",
    "missing_comments = filtered_df[\"Comment\"].isna().sum()\n",
    "\n",
    "print(\"‚úÖ Comments mapped with status condition.\")\n",
    "print(f\"Missing Comments (Active only): {missing_comments} out of {total_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65b596e-801c-4cd0-aa1a-3e05becafed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Net Needed QTY calculated.\n",
      "‚úÖ Value column calculated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==========================================================\n",
    "# BLOCK 15 ‚Äì CALCULATE NET NEEDED QTY (Excel Logic)\n",
    "# ==========================================================\n",
    "\n",
    "# Ensure numeric inputs (avoid errors like IFERROR)\n",
    "num_cols = [\"OUTL_QTY\", \"SOHQty\", \"POonOrdrQty\", \"TransferonOrderQty\", \"Set_Qty\"]\n",
    "for c in num_cols:\n",
    "    if c in filtered_df.columns:\n",
    "        filtered_df[c] = pd.to_numeric(filtered_df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# If your \"UNIT SIZE\" column in Excel corresponds to Set_Qty in Python:\n",
    "# (If you named it differently, change here)\n",
    "filtered_df[\"UNIT SIZE\"] = filtered_df[\"Set_Qty\"]\n",
    "\n",
    "# Supply = SUM(MAX(AT,0), MAX(AV,0), MAX(AW,0))\n",
    "supply = (\n",
    "    filtered_df[\"SOHQty\"].clip(lower=0) +\n",
    "    filtered_df[\"POonOrdrQty\"].clip(lower=0) +\n",
    "    filtered_df[\"TransferonOrderQty\"].clip(lower=0)\n",
    ")\n",
    "\n",
    "# Raw need = OUTL_QTY - Supply\n",
    "raw_need = filtered_df[\"OUTL_QTY\"] - supply\n",
    "\n",
    "# Need = IFERROR(MAX(ROUNDUP(raw_need,0),0),0)\n",
    "need_units = np.ceil(raw_need).clip(lower=0)\n",
    "\n",
    "need_mult_6 = (np.floor(need_units / 6 + 0.5) * 6)\n",
    "\n",
    "# Final Net Needed QTY (Excel IF condition)\n",
    "filtered_df[\"Net Needed QTY\"] = np.where(\n",
    "    filtered_df[\"UNIT SIZE\"] == 1,\n",
    "    need_mult_6,\n",
    "    need_units\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Net Needed QTY calculated.\")\n",
    "\n",
    "filtered_df[\"Value\"] = (\n",
    "    pd.to_numeric(filtered_df[\"NetVndrUnitCost\"], errors=\"coerce\").fillna(0)\n",
    "    *\n",
    "    pd.to_numeric(filtered_df[\"Net Needed QTY\"], errors=\"coerce\").fillna(0)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Value column calculated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc94a3e8-6eb1-4654-8ed3-337772c0aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final output ready | Rows: 17731 | Columns: 30\n",
      "‚úÖ File saved as CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==========================================================\n",
    "# BLOCK 16 ‚Äì FINAL OUTPUT VALIDATION + EXPORT\n",
    "# ==========================================================\n",
    "\n",
    "# 0) Clean any hidden spaces in column names (very common)\n",
    "filtered_df.columns = filtered_df.columns.str.strip()\n",
    "\n",
    "# 1) These are the columns (as-is) that represent your required output data\n",
    "#    (No renaming; we just pick the correct columns that hold the right meaning)\n",
    "final_cols = [\n",
    "    \"Store\", \"StoreName\", \"SKU\", \"SkuDesc\", \"SkuStatus\",\n",
    "    \"Supplier\", \"SupplierName\", \"SupplierType\",\n",
    "    \"Dept.\", \"DeptName\", \"SubDeptName\", \"ClassName\", \"SubClassName\",\n",
    "    \"Brand\", \"BrandName\",\n",
    "    \"SOHQty\", \"InvValue\", \"POonOrdrQty\",\n",
    "    \"Adj_30Days\", \"Adj_60Days\", \"Adj_90Days\",          # = 30 Days P1/P2/P3 logic\n",
    "    \"Avg_30_60_90\",                                   # = AVG Sold QTY\n",
    "    \"Daily_Avg_Sales\",                                # = ADF\n",
    "    \"DeptHierarchy\",                                  # = Con.\n",
    "    \"OUTL_901_Days\",                                  # = OUTL Days\n",
    "    \"OUTL_QTY\",                                       # = OUTL QTY\n",
    "    \"UNIT SIZE\",                                      # = UNIT SIZE\n",
    "    \"Net Needed QTY\",                                 # = Net Needed QTY\n",
    "    \"Value\",                                          # = Value\n",
    "    \"Comment\"                                         # = Comments (your column name)\n",
    "]\n",
    "\n",
    "\n",
    "final_df = filtered_df[final_cols].copy()\n",
    "\n",
    "print(f\"‚úÖ Final output ready | Rows: {final_df.shape[0]} | Columns: {final_df.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "output_path = os.path.join(base_path, \"Processed\", \"Master_to_Send.csv\")\n",
    "\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"‚úÖ File saved as CSV successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b844554-c02d-4b91-a30e-0f7316f5c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data preparation completed ‚Äî non-replenishment + zero-need rows removed.\n",
      "Remaining rows: 672\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 17 ‚Äì Excludes DSD, Manual PO, Inactive, Blank, 0 Needed\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "\n",
    "NEED_COL = \"Net Needed QTY\"\n",
    "\n",
    "# Ensure Needed Qty is numeric\n",
    "final_df[NEED_COL] = pd.to_numeric(final_df[NEED_COL], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Create clean version of Comment for safe filtering\n",
    "comment_clean = (\n",
    "    final_df[\"Comment\"]\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Define comments to exclude\n",
    "Ex_comments = {\"dsd\", \"manual po\", \"inactive\"}\n",
    "\n",
    "# Keep only valid replenishment rows\n",
    "final_df = final_df[\n",
    "    comment_clean.notna()               # remove null comments\n",
    "    & (comment_clean != \"\")             # remove blank comments\n",
    "    & (~comment_clean.isin(Ex_comments))# remove excluded types\n",
    "    & (final_df[NEED_COL] > 0)          # ‚úÖ remove zero-needed qty\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Data preparation completed ‚Äî non-replenishment + zero-need rows removed.\")\n",
    "print(f\"Remaining rows: {len(final_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ecd9002-e3f1-4bda-a851-c5ac2ddbfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Buyer column assigned (exact match rule applied).\n",
      "Rows where Buyer is NULL or blank:\n",
      "Empty DataFrame\n",
      "Columns: [Store, StoreName, SKU, SkuDesc, SkuStatus, Supplier, SupplierName, SupplierType, Dept., DeptName, SubDeptName, ClassName, SubClassName, Brand, BrandName, SOHQty, InvValue, POonOrdrQty, Adj_30Days, Adj_60Days, Adj_90Days, Avg_30_60_90, Daily_Avg_Sales, DeptHierarchy, OUTL_901_Days, OUTL_QTY, UNIT SIZE, Net Needed QTY, Value, Comment, Buyer]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 18 ‚Äì Adding Buyer Code as per comments\n",
    "# ==========================================================\n",
    "\n",
    "final_df[\"Buyer\"] = \"\"\n",
    "\n",
    "final_df.loc[\n",
    "    final_df[\"Comment\"].str.startswith(\"By Segment\", na=False),\n",
    "    \"Buyer\"\n",
    "] = \"ASCO\"\n",
    "\n",
    "final_df.loc[\n",
    "    final_df[\"Comment\"].str.strip().str.lower().eq(\"under buyer id\"),\n",
    "    \"Buyer\"\n",
    "] = \"A002\"\n",
    "\n",
    "print(\"‚úÖ Buyer column assigned (exact match rule applied).\")\n",
    "\n",
    "null_or_blank = (\n",
    "    final_df[\"Buyer\"].isna() |\n",
    "    (final_df[\"Buyer\"].str.strip() == \"\")\n",
    ")\n",
    "\n",
    "print(\"Rows where Buyer is NULL or blank:\")\n",
    "print(final_df[null_or_blank].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "184089fd-22c7-4ed4-bc6f-3bed34a3ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rows with PO_Value < 10 removed.\n",
      "Remaining rows: 621\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 19 ‚Äì Calculating PO value and Filter-out < 10 SAR\n",
    "# ==========================================================\n",
    "\n",
    "final_df[\"PO_Value\"] = (\n",
    "    final_df\n",
    "    .groupby(\n",
    "        [\"Buyer\", \"Comment\", \"Supplier\"],\n",
    "        dropna=False\n",
    "    )[\"Value\"]\n",
    "    .transform(\"sum\")\n",
    ")\n",
    "\n",
    "final_df = final_df[final_df[\"PO_Value\"] >= 10]\n",
    "\n",
    "print(\"‚úÖ Rows with PO_Value < 10 removed.\")\n",
    "print(f\"Remaining rows: {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de129e0f-70e1-4d29-bd33-4c163f6fed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bahrain Minimum loaded | Rows: 149 | Columns: 9\n",
      "‚úÖ MOQ FOR BAHRAIN loaded | Rows: 189 | Columns: 2\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 19 ‚Äì Loading Piece SKU and MOQ Baharain Files\n",
    "# ==========================================================\n",
    "\n",
    "min_path = os.path.join(reference_folder, \"Bahrain Minimum.xlsx\")\n",
    "\n",
    "minimum_df = pd.read_excel(min_path)\n",
    "\n",
    "print(f\"‚úÖ Bahrain Minimum loaded | Rows: {minimum_df.shape[0]} | Columns: {minimum_df.shape[1]}\")\n",
    "\n",
    "moq_path = os.path.join(reference_folder, \"MOQ FOR BAHRAIN.xlsx\")\n",
    "\n",
    "moq_df = pd.read_excel(moq_path)\n",
    "\n",
    "print(f\"‚úÖ MOQ FOR BAHRAIN loaded | Rows: {moq_df.shape[0]} | Columns: {moq_df.shape[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5bc59a-3d38-4acb-9317-61aebec4af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bahrain Minimum matches: 1 out of 621\n",
      "‚úÖ SKU swapped (Carton ‚Üí Piece) and Net Needed QTY multiplied by Min Qty.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 19 ‚Äì Converting Box to Piece SKU where neeeded\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "FINAL_SKU_COL = \"SKU\"\n",
    "FINAL_NEED_COL = \"Net Needed QTY\"\n",
    "\n",
    "MIN_CARTON_COL = \"Ctn Code\"\n",
    "MIN_PIECE_COL  = \"Pc Cpde\"\n",
    "MIN_QTY_COL    = \"Min Qty\"\n",
    "\n",
    "# ----------------------------\n",
    "# Clean / standardize keys\n",
    "# ----------------------------\n",
    "final_df[FINAL_SKU_COL] = final_df[FINAL_SKU_COL].astype(\"string\").str.strip()\n",
    "minimum_df[MIN_CARTON_COL] = minimum_df[MIN_CARTON_COL].astype(\"string\").str.strip()\n",
    "minimum_df[MIN_PIECE_COL] = minimum_df[MIN_PIECE_COL].astype(\"string\").str.strip()\n",
    "\n",
    "# Ensure numeric qty fields\n",
    "final_df[FINAL_NEED_COL] = pd.to_numeric(final_df[FINAL_NEED_COL], errors=\"coerce\").fillna(0)\n",
    "minimum_df[MIN_QTY_COL]  = pd.to_numeric(minimum_df[MIN_QTY_COL], errors=\"coerce\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build lookup from Bahrain Minimum: Carton_SKU -> (Piece_SKU, Min Qty)\n",
    "# ----------------------------\n",
    "min_map = (\n",
    "    minimum_df\n",
    "    .dropna(subset=[MIN_CARTON_COL])\n",
    "    .drop_duplicates(subset=[MIN_CARTON_COL])\n",
    "    .set_index(MIN_CARTON_COL)[[MIN_PIECE_COL, MIN_QTY_COL]]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Map Piece SKU + Min Qty into final_df\n",
    "# ----------------------------\n",
    "final_df[\"Mapped_Piece_SKU\"] = final_df[FINAL_SKU_COL].map(min_map[MIN_PIECE_COL])\n",
    "final_df[\"Mapped_Min_Qty\"]   = final_df[FINAL_SKU_COL].map(min_map[MIN_QTY_COL])\n",
    "\n",
    "matched = final_df[\"Mapped_Piece_SKU\"].notna() & final_df[\"Mapped_Min_Qty\"].notna()\n",
    "\n",
    "print(f\"‚úÖ Bahrain Minimum matches: {matched.sum():,} out of {len(final_df):,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Apply updates ONLY for matched rows\n",
    "# ----------------------------\n",
    "# Swap SKU (Carton SKU -> Piece SKU)\n",
    "final_df.loc[matched, FINAL_SKU_COL] = final_df.loc[matched, \"Mapped_Piece_SKU\"]\n",
    "\n",
    "# Update Needed Qty: Needed * Min Qty\n",
    "final_df.loc[matched, FINAL_NEED_COL] = (\n",
    "    final_df.loc[matched, FINAL_NEED_COL] * final_df.loc[matched, \"Mapped_Min_Qty\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SKU swapped (Carton ‚Üí Piece) and Net Needed QTY multiplied by Min Qty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a93eccb-b6be-450a-a8ee-6713a9fd4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MOQ matched rows: 12 out of 621\n",
      "‚úÖ Net Needed QTY overwritten using MOQ rules\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 20 ‚Äì Checking and Updating MOQ lines\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Column names ----\n",
    "FINAL_SKU_COL = \"SKU\"\n",
    "NEED_COL = \"Net Needed QTY\"\n",
    "\n",
    "MOQ_SKU_COL = \"Danube Code\"     # ‚úÖ MOQ file SKU column\n",
    "MOQ_COL = \"MOQ\"          # change if your MOQ qty column name differs\n",
    "\n",
    "MAX_EXTRA_ALLOWED = 12\n",
    "\n",
    "# ---- Clean keys ----\n",
    "final_df[FINAL_SKU_COL] = final_df[FINAL_SKU_COL].astype(\"string\").str.strip()\n",
    "moq_df[MOQ_SKU_COL]     = moq_df[MOQ_SKU_COL].astype(\"string\").str.strip()\n",
    "\n",
    "# ---- Ensure numeric ----\n",
    "final_df[NEED_COL] = pd.to_numeric(final_df[NEED_COL], errors=\"coerce\").fillna(0)\n",
    "moq_df[MOQ_COL]    = pd.to_numeric(moq_df[MOQ_COL], errors=\"coerce\")\n",
    "\n",
    "# ---- Build lookup: SKU2 -> MOQ ----\n",
    "moq_lookup = (\n",
    "    moq_df\n",
    "    .dropna(subset=[MOQ_COL])\n",
    "    .drop_duplicates(subset=[MOQ_SKU_COL])\n",
    "    .set_index(MOQ_SKU_COL)[MOQ_COL]\n",
    ")\n",
    "\n",
    "# ---- Map MOQ into final_df using SKU ----\n",
    "final_df[\"Mapped_MOQ\"] = final_df[FINAL_SKU_COL].map(moq_lookup)\n",
    "\n",
    "matched = (\n",
    "    final_df[\"Mapped_MOQ\"].notna()\n",
    "    & (final_df[\"Mapped_MOQ\"] > 0)\n",
    "    & (final_df[NEED_COL] > 0)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ MOQ matched rows: {matched.sum():,} out of {len(final_df):,}\")\n",
    "\n",
    "# ---- Apply MOQ rounding rule (round UP to nearest MOQ multiple) ----\n",
    "need = final_df.loc[matched, NEED_COL]\n",
    "moq  = final_df.loc[matched, \"Mapped_MOQ\"]\n",
    "\n",
    "adjusted = np.ceil(need / moq) * moq\n",
    "extra = adjusted - need\n",
    "\n",
    "# ---- If increase > 12 then set to 0 ----\n",
    "adjusted_final = np.where(extra > MAX_EXTRA_ALLOWED, 0, adjusted)\n",
    "\n",
    "# ---- OVERWRITE Net Needed QTY (matched rows only) ----\n",
    "final_df.loc[matched, NEED_COL] = adjusted_final\n",
    "\n",
    "print(\"‚úÖ Net Needed QTY overwritten using MOQ rules\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd6baac-33cf-4a28-a592-f0ae043b1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns reordered successfully.\n",
      "‚úÖ File saved as CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCK 21 ‚Äì Data Export\n",
    "# ==========================================================\n",
    "\n",
    "\n",
    "# Columns required at the beginning\n",
    "priority_cols = [\n",
    "    \"Buyer\",\n",
    "    \"Comment\",\n",
    "    \"Store\",\n",
    "    \"Supplier\",\n",
    "    \"SKU\",\n",
    "    \"Net Needed QTY\"\n",
    "]\n",
    "\n",
    "# Keep only those that actually exist (safety check)\n",
    "priority_cols = [c for c in priority_cols if c in final_df.columns]\n",
    "\n",
    "# Remaining columns (preserve order)\n",
    "remaining_cols = [c for c in final_df.columns if c not in priority_cols]\n",
    "\n",
    "# Reorder DataFrame\n",
    "final_df = final_df[priority_cols + remaining_cols]\n",
    "\n",
    "print(\"‚úÖ Columns reordered successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "output_path = os.path.join(base_path, \"Processed\", \"Upload_File.csv\")\n",
    "\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"‚úÖ File saved as CSV successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
